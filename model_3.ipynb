{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import sklearn as sk\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract video frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' make the list of paths to the videos\\n    extract the frames from the videos into images (what if i just extract two frames from each?)\\n    save these images in a new folder\\n    this folder is now your data dir\\n    make sure that each frame is in its respective folder (violence, non violence)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG_SIZE = 224\n",
    "\n",
    "v1 = \"video_data/violence_dataset/Violence\"\n",
    "v2 = \"video_data/surveillance_dataset/fight\"\n",
    "\n",
    "n1 = \"video_data/violence_dataset/NonViolence\"\n",
    "n2 = \"video_data/surveillance_dataset/nofight\"\n",
    "violent_videos = [v1, v2]\n",
    "nonviolent_videos = [n1, n2]\n",
    "\n",
    "\n",
    "NEW_FOLDER = \"new_data_dir\"\n",
    "\n",
    "nonviolent_path = \"video_data/new_data_dir/NonViolent\"\n",
    "violent_path = \"video_data/new_data_dir/Violent\"\n",
    "''' make the list of paths to the videos\n",
    "    extract the frames from the videos into images (what if i just extract two frames from each?)\n",
    "    save these images in a new folder\n",
    "    this folder is now your data dir\n",
    "    make sure that each frame is in its respective folder (violence, non violence)'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have video_to_frames(path) function, what I need to do is use [violent_videos, nonviolent_videos] to automatically go into NEW_FOLDER accordingly their new folders with the frames extracted already\n",
    "\n",
    "so change video_to_frames(list) to take in a list of lists or strings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "video_to_frames functions, takes in a path and extracts frames to outpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_frames(PARENT_PATHS: list[str], out_path:str):\n",
    "    count = 0\n",
    "    for path in PARENT_PATHS: # path is already the needed directory, just walk through it for each video\n",
    "    \n",
    "            \n",
    "        assert os.path.exists(path)\n",
    "        \n",
    "        for i, video in tqdm(enumerate(os.listdir(path)), total=len(os.listdir(path))):\n",
    "\n",
    "            \n",
    "            #video_to_frames(path + '/' + video, out_path)\n",
    "\n",
    "            video = os.path.join(path, video)\n",
    "            assert os.path.exists(video)\n",
    "            \n",
    "\n",
    "            cap = cv2.VideoCapture(video)\n",
    "            assert cap\n",
    "\n",
    "            assert cap.isOpened()\n",
    "\n",
    "            while cap.isOpened():\n",
    "                \n",
    "                success, image = cap.read()\n",
    "                ID = int(cap.get(1))\n",
    "\n",
    "                n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                mid = int(n_frames/2)\n",
    "                first_q = int(n_frames/4)\n",
    "                third_q = int(n_frames * .75)\n",
    "                end = n_frames\n",
    "                \n",
    "                #assert success\n",
    "\n",
    "                if success:\n",
    "\n",
    "                \n",
    "                    if ID == first_q or ID == mid or ID == third_q or ID == end:\n",
    "                        \n",
    "                        \n",
    "                        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "                        #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                        \n",
    "\n",
    "                        frame_name = 'Frame'+str(count)+'.jpg'\n",
    "                        cv2.imwrite(os.path.join(out_path, frame_name), image)\n",
    "                        count+=1\n",
    "\n",
    "                    else: continue\n",
    "        \n",
    "                    \n",
    "\n",
    "                else: \n",
    "                    break\n",
    "            cap.release()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:16<00:00, 13.07it/s]\n",
      "100%|██████████| 150/150 [00:11<00:00, 12.74it/s]\n"
     ]
    }
   ],
   "source": [
    "video_to_frames(nonviolent_videos, nonviolent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:29<00:00,  6.68it/s]\n",
      "100%|██████████| 150/150 [00:09<00:00, 15.62it/s]\n"
     ]
    }
   ],
   "source": [
    "video_to_frames(violent_videos, violent_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BENCHMARK: preproccessed 2300 videos to images in less 60s\n",
    "update: 58s to 15+18 s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extracting frames from each path and placing them in their new respective directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of non violent: 4600, length of violent: 4600 \n"
     ]
    }
   ],
   "source": [
    "print(f\"length of non violent: {len(os.listdir(nonviolent_path))}, length of violent: {len(os.listdir(violent_path))} \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Tensorflow to create the x_train, y_train, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting split-folders\n",
      "  Using cached split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
      "Installing collected packages: split-folders\n",
      "Successfully installed split-folders-0.5.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install split-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 9200 files [00:54, 168.80 files/s]\n"
     ]
    }
   ],
   "source": [
    "import splitfolders\n",
    "split_data = \"SPLIT_DATA\"\n",
    "new_data = \"video_data/new_data_dir\"\n",
    "splitfolders.ratio(new_data, split_data, seed=42, ratio = (.8, .2), group_prefix = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7360 images belonging to 2 classes.\n",
      "Found 1840 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "valid_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "train_dir = 'SPLIT_DATA/train/'\n",
    "test_dir = 'SPLIT_DATA/val/'\n",
    "\n",
    "train_data = train_datagen.flow_from_directory(train_dir,\n",
    "                                               batch_size=16, # number of images to process at a time \n",
    "                                               target_size=(224, 224), # convert all images to be 224 x 224\n",
    "                                               class_mode=\"binary\", # type of problem we're working on\n",
    "                                               seed=42)\n",
    "\n",
    "valid_data = valid_datagen.flow_from_directory(test_dir,\n",
    "                                               batch_size=16,\n",
    "                                               target_size=(224, 224),\n",
    "                                               class_mode=\"binary\",\n",
    "                                               seed=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one below, keeping for x,y data lists to np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "\n",
    "\n",
    "data_augmentation = Sequential(\n",
    "    [\n",
    "        layers.RandomRotation(factor=0.15),\n",
    "        layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "        layers.RandomFlip(),\n",
    "        layers.RandomContrast(factor=0.1),\n",
    "    ],\n",
    "    name=\"img_augmentation\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.efficientnet_v2 import EfficientNetV2B0\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet \n",
    "class OwnEfficientNetB0:\n",
    "    @staticmethod\n",
    "    def build(input_shape, data_augmentation, trainable=False, dropout=0.3):\n",
    "        inputs = keras.Input(shape=input_shape)\n",
    "        x = data_augmentation(inputs)\n",
    "        x = preprocess_input(x)\n",
    "\n",
    "        baseModel = MobileNet(weights=\"imagenet\", include_top=False, input_tensor=x)\n",
    "        baseModel.trainable = trainable\n",
    "\n",
    "        headModel = baseModel.output\n",
    "        headModel = layers.GlobalAveragePooling2D()(headModel)\n",
    "        headModel = layers.Dropout(dropout)(headModel)\n",
    "        outputs = layers.Dense(1, activation=\"sigmoid\")(headModel)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "model = OwnEfficientNetB0.build(input_shape=(IMG_SIZE,IMG_SIZE) + (3,), data_augmentation=data_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep batchlayers intact, reduces learning time\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, layers.BatchNormalization):\n",
    "        layer.trainable = False\n",
    "    else:\n",
    "        layer.trainable = True\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.00001),\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "            metrics=keras.metrics.BinaryAccuracy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=30, mode='min', min_delta=0.0001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, epochs=25, steps_per_epoch=len(train_data))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba3a49a2d89a21dd1fa2c7b5d6b7fcd2a31de042e8686c95cc798c595b19bacd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
